main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py:162: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
| distributed init (rank 0): env://
ngpus_per_node 1
Use GPU: 0 for training
=> creating model 'resnet50'
DistributedDataParallel(
  (module): SimSiam(
    (encoder): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Sequential(
        (0): Linear(in_features=2048, out_features=2048, bias=False)
        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=2048, out_features=2048, bias=False)
        (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=2048, out_features=2048, bias=True)
        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (predictor): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=2048, bias=True)
    )
  )
)
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Length of train_dataset =  126689
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Epoch: [0][  0/625]	Time  3.275 ( 3.275)	Data  2.584 ( 2.584)	Loss 2.3758e+00 (2.3758e+00)
Epoch: [0][ 10/625]	Time  0.027 ( 0.321)	Data  0.006 ( 0.240)	Loss 1.8991e+00 (2.3330e+00)
Epoch: [0][ 20/625]	Time  0.025 ( 0.181)	Data  0.005 ( 0.128)	Loss 2.2702e+00 (2.0913e+00)
Epoch: [0][ 30/625]	Time  0.027 ( 0.131)	Data  0.006 ( 0.089)	Loss 2.0605e+00 (2.0685e+00)
Epoch: [0][ 40/625]	Time  0.027 ( 0.106)	Data  0.006 ( 0.069)	Loss 1.2470e+00 (1.9145e+00)
Epoch: [0][ 50/625]	Time  0.069 ( 0.092)	Data  0.002 ( 0.056)	Loss 1.1772e+00 (1.8608e+00)
Epoch: [0][ 60/625]	Time  0.027 ( 0.085)	Data  0.003 ( 0.048)	Loss 7.3574e-01 (1.7589e+00)
Epoch: [0][ 70/625]	Time  0.026 ( 0.077)	Data  0.007 ( 0.042)	Loss 9.3448e-01 (1.6972e+00)
Epoch: [0][ 80/625]	Time  0.027 ( 0.070)	Data  0.006 ( 0.037)	Loss 1.2349e+00 (1.6268e+00)
Epoch: [0][ 90/625]	Time  0.027 ( 0.066)	Data  0.007 ( 0.034)	Loss 8.8093e-01 (1.5526e+00)
Epoch: [0][100/625]	Time  0.025 ( 0.062)	Data  0.006 ( 0.031)	Loss 9.0487e-01 (1.4702e+00)
Epoch: [0][110/625]	Time  0.025 ( 0.059)	Data  0.006 ( 0.029)	Loss 7.1083e-01 (1.4050e+00)
Epoch: [0][120/625]	Time  0.027 ( 0.056)	Data  0.007 ( 0.027)	Loss 6.6669e-01 (1.3501e+00)
Epoch: [0][130/625]	Time  0.027 ( 0.054)	Data  0.007 ( 0.026)	Loss 2.7378e-01 (1.2898e+00)
Epoch: [0][140/625]	Time  0.025 ( 0.052)	Data  0.006 ( 0.024)	Loss 5.5597e-01 (1.2336e+00)
Epoch: [0][150/625]	Time  0.027 ( 0.050)	Data  0.006 ( 0.023)	Loss 3.2160e-01 (1.1755e+00)
Epoch: [0][160/625]	Time  0.027 ( 0.049)	Data  0.006 ( 0.022)	Loss 4.1778e-01 (1.1196e+00)
Epoch: [0][170/625]	Time  0.027 ( 0.047)	Data  0.007 ( 0.021)	Loss 6.5209e-01 (1.0723e+00)
Epoch: [0][180/625]	Time  0.027 ( 0.046)	Data  0.007 ( 0.020)	Loss 2.7221e-01 (1.0226e+00)
Epoch: [0][190/625]	Time  0.026 ( 0.045)	Data  0.007 ( 0.020)	Loss 3.7843e-01 (9.8802e-01)
Epoch: [0][200/625]	Time  0.026 ( 0.044)	Data  0.007 ( 0.019)	Loss 1.5140e-01 (9.5192e-01)
Epoch: [0][210/625]	Time  0.029 ( 0.043)	Data  0.007 ( 0.018)	Loss 1.1390e-01 (9.1812e-01)
Epoch: [0][220/625]	Time  0.026 ( 0.043)	Data  0.007 ( 0.018)	Loss 3.0728e-01 (8.8422e-01)
Epoch: [0][230/625]	Time  0.026 ( 0.042)	Data  0.007 ( 0.017)	Loss 2.5899e-01 (8.5151e-01)
Epoch: [0][240/625]	Time  0.025 ( 0.041)	Data  0.006 ( 0.017)	Loss 2.1857e-01 (8.2457e-01)
Epoch: [0][250/625]	Time  0.026 ( 0.041)	Data  0.006 ( 0.017)	Loss 1.4606e-01 (7.9622e-01)
Epoch: [0][260/625]	Time  0.026 ( 0.040)	Data  0.007 ( 0.016)	Loss 1.2718e-01 (7.7232e-01)
Epoch: [0][270/625]	Time  0.025 ( 0.040)	Data  0.006 ( 0.016)	Loss 6.6230e-02 (7.4568e-01)
Epoch: [0][280/625]	Time  0.027 ( 0.039)	Data  0.008 ( 0.015)	Loss 1.8957e-01 (7.2376e-01)
Epoch: [0][290/625]	Time  0.026 ( 0.039)	Data  0.007 ( 0.015)	Loss 5.9588e-02 (7.0213e-01)
Epoch: [0][300/625]	Time  0.051 ( 0.039)	Data  0.006 ( 0.015)	Loss 1.9017e-01 (6.8317e-01)
Epoch: [0][310/625]	Time  0.025 ( 0.038)	Data  0.007 ( 0.015)	Loss 1.7247e-01 (6.6828e-01)
Epoch: [0][320/625]	Time  0.027 ( 0.038)	Data  0.007 ( 0.014)	Loss 4.5236e-02 (6.5027e-01)
Epoch: [0][330/625]	Time  0.027 ( 0.038)	Data  0.007 ( 0.014)	Loss 1.8529e-01 (6.3383e-01)
Epoch: [0][340/625]	Time  0.027 ( 0.038)	Data  0.007 ( 0.014)	Loss 6.3937e-02 (6.1784e-01)
Epoch: [0][350/625]	Time  0.026 ( 0.037)	Data  0.007 ( 0.014)	Loss 3.6416e-02 (6.0210e-01)
Epoch: [0][360/625]	Time  0.026 ( 0.037)	Data  0.006 ( 0.014)	Loss 3.2321e-02 (5.8693e-01)
Epoch: [0][370/625]	Time  0.024 ( 0.037)	Data  0.006 ( 0.013)	Loss 1.7119e-01 (5.7261e-01)
Epoch: [0][380/625]	Time  0.047 ( 0.036)	Data  0.026 ( 0.013)	Loss 4.5662e-02 (5.5880e-01)
Epoch: [0][390/625]	Time  0.023 ( 0.036)	Data  0.005 ( 0.013)	Loss 2.4705e-02 (5.4664e-01)
Epoch: [0][400/625]	Time  0.026 ( 0.036)	Data  0.007 ( 0.013)	Loss 5.8831e-02 (5.3438e-01)
Epoch: [0][410/625]	Time  0.027 ( 0.036)	Data  0.007 ( 0.013)	Loss 1.1775e-01 (5.2258e-01)
Epoch: [0][420/625]	Time  0.027 ( 0.036)	Data  0.006 ( 0.013)	Loss 1.7299e-01 (5.1180e-01)
Epoch: [0][430/625]	Time  0.026 ( 0.036)	Data  0.007 ( 0.012)	Loss 2.8219e-02 (5.0219e-01)
Epoch: [0][440/625]	Time  0.043 ( 0.036)	Data  0.007 ( 0.012)	Loss 4.5021e-02 (4.9164e-01)
Epoch: [0][450/625]	Time  0.026 ( 0.035)	Data  0.007 ( 0.012)	Loss 8.2881e-02 (4.8401e-01)
Epoch: [0][460/625]	Time  0.027 ( 0.035)	Data  0.007 ( 0.012)	Loss 7.8720e-02 (4.7442e-01)
Epoch: [0][470/625]	Time  0.027 ( 0.035)	Data  0.007 ( 0.012)	Loss 3.9840e-02 (4.6541e-01)
Epoch: [0][480/625]	Time  0.026 ( 0.035)	Data  0.006 ( 0.012)	Loss 4.7998e-02 (4.5717e-01)
Epoch: [0][490/625]	Time  0.026 ( 0.035)	Data  0.007 ( 0.012)	Loss 3.3278e-02 (4.4923e-01)
Epoch: [0][500/625]	Time  0.049 ( 0.034)	Data  0.007 ( 0.012)	Loss 5.1532e-02 (4.4125e-01)
Epoch: [0][510/625]	Time  0.027 ( 0.034)	Data  0.007 ( 0.012)	Loss 3.0340e-02 (4.3357e-01)
Epoch: [0][520/625]	Time  0.026 ( 0.034)	Data  0.007 ( 0.012)	Loss 5.6545e-02 (4.2598e-01)
Epoch: [0][530/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.011)	Loss 2.8845e-02 (4.1881e-01)
Epoch: [0][540/625]	Time  0.027 ( 0.034)	Data  0.007 ( 0.011)	Loss 8.7854e-02 (4.1197e-01)
Epoch: [0][550/625]	Time  0.026 ( 0.034)	Data  0.007 ( 0.011)	Loss 2.2978e-01 (4.0623e-01)
Epoch: [0][560/625]	Time  0.026 ( 0.034)	Data  0.007 ( 0.011)	Loss 2.8696e-02 (4.0069e-01)
Epoch: [0][570/625]	Time  0.026 ( 0.034)	Data  0.007 ( 0.011)	Loss 5.8847e-02 (3.9448e-01)
Epoch: [0][580/625]	Time  0.026 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.5091e-01 (3.8898e-01)
Epoch: [0][590/625]	Time  0.026 ( 0.033)	Data  0.007 ( 0.011)	Loss 5.9592e-02 (3.8362e-01)
Epoch: [0][600/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 2.8940e-02 (3.7775e-01)
Epoch: [0][610/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.9059e-01 (3.7240e-01)
Epoch: [0][620/625]	Time  0.026 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.9017e-02 (3.6711e-01)
Test: [  0/625]	Time  2.621 ( 2.621)	Loss 3.5868e-02 (3.5868e-02)
Test: [ 10/625]	Time  0.075 ( 0.264)	Loss 4.1004e-02 (3.7043e-02)
Test: [ 20/625]	Time  0.010 ( 0.149)	Loss 2.5530e-02 (3.1565e-02)
Test: [ 30/625]	Time  0.010 ( 0.107)	Loss 2.9860e-02 (3.1110e-02)
Test: [ 40/625]	Time  0.010 ( 0.086)	Loss 1.8355e-02 (2.8726e-02)
Test: [ 50/625]	Time  0.010 ( 0.075)	Loss 1.9564e-02 (3.4646e-02)
Test: [ 60/625]	Time  0.011 ( 0.067)	Loss 3.3404e-02 (3.4289e-02)
Test: [ 70/625]	Time  0.082 ( 0.063)	Loss 2.4855e-02 (3.4442e-02)
Test: [ 80/625]	Time  0.010 ( 0.058)	Loss 4.4740e-02 (3.4752e-02)
Test: [ 90/625]	Time  0.064 ( 0.055)	Loss 3.8981e-02 (3.4415e-02)
Test: [100/625]	Time  0.056 ( 0.051)	Loss 3.3086e-02 (3.3833e-02)
Test: [110/625]	Time  0.010 ( 0.049)	Loss 2.2911e-02 (3.3341e-02)
Test: [120/625]	Time  0.010 ( 0.046)	Loss 3.4285e-02 (3.4159e-02)
Test: [130/625]	Time  0.010 ( 0.044)	Loss 2.5883e-02 (3.4012e-02)
Test: [140/625]	Time  0.010 ( 0.043)	Loss 2.9795e-02 (3.3746e-02)
Test: [150/625]	Time  0.011 ( 0.041)	Loss 2.2885e-02 (3.3625e-02)
Test: [160/625]	Time  0.011 ( 0.040)	Loss 3.4483e-02 (3.3137e-02)
Test: [170/625]	Time  0.010 ( 0.039)	Loss 5.9089e-02 (3.3271e-02)
Test: [180/625]	Time  0.010 ( 0.038)	Loss 2.3661e-02 (3.2975e-02)
Test: [190/625]	Time  0.012 ( 0.037)	Loss 3.7261e-02 (3.2879e-02)
Test: [200/625]	Time  0.010 ( 0.036)	Loss 4.7596e-02 (3.3269e-02)
Test: [210/625]	Time  0.074 ( 0.036)	Loss 1.9447e-02 (3.3362e-02)
Test: [220/625]	Time  0.054 ( 0.035)	Loss 3.9304e-02 (3.2993e-02)
Test: [230/625]	Time  0.058 ( 0.035)	Loss 4.2037e-02 (3.2879e-02)
Test: [240/625]	Time  0.055 ( 0.034)	Loss 2.3558e-02 (3.2904e-02)
Test: [250/625]	Time  0.010 ( 0.033)	Loss 2.4351e-02 (3.2727e-02)
Test: [260/625]	Time  0.010 ( 0.033)	Loss 5.4367e-02 (3.2726e-02)
Test: [270/625]	Time  0.075 ( 0.033)	Loss 2.9550e-02 (3.2516e-02)
Test: [280/625]	Time  0.010 ( 0.032)	Loss 3.4148e-02 (3.2416e-02)
Test: [290/625]	Time  0.011 ( 0.032)	Loss 2.6035e-02 (3.2339e-02)
Test: [300/625]	Time  0.010 ( 0.032)	Loss 3.7092e-02 (3.2444e-02)
Test: [310/625]	Time  0.072 ( 0.032)	Loss 2.7834e-02 (3.2372e-02)
Test: [320/625]	Time  0.010 ( 0.031)	Loss 3.0920e-02 (3.2565e-02)
Test: [330/625]	Time  0.010 ( 0.031)	Loss 4.5353e-02 (3.2699e-02)
Test: [340/625]	Time  0.065 ( 0.031)	Loss 2.5287e-02 (3.2687e-02)
Test: [350/625]	Time  0.063 ( 0.030)	Loss 2.7199e-02 (3.2841e-02)
Test: [360/625]	Time  0.010 ( 0.030)	Loss 2.2777e-02 (3.2827e-02)
Test: [370/625]	Time  0.010 ( 0.030)	Loss 4.3602e-02 (3.2693e-02)
Test: [380/625]	Time  0.010 ( 0.030)	Loss 2.5119e-02 (3.2599e-02)
Test: [390/625]	Time  0.012 ( 0.030)	Loss 1.9914e-02 (3.2740e-02)
Test: [400/625]	Time  0.012 ( 0.029)	Loss 2.6715e-02 (3.2613e-02)
Test: [410/625]	Time  0.010 ( 0.029)	Loss 3.9515e-02 (3.2482e-02)
Test: [420/625]	Time  0.010 ( 0.029)	Loss 5.4061e-02 (3.2481e-02)
Test: [430/625]	Time  0.014 ( 0.029)	Loss 2.8858e-02 (3.2661e-02)
Test: [440/625]	Time  0.011 ( 0.029)	Loss 2.8077e-02 (3.2593e-02)
Test: [450/625]	Time  0.010 ( 0.029)	Loss 3.3038e-02 (3.2713e-02)
Test: [460/625]	Time  0.063 ( 0.029)	Loss 2.1925e-02 (3.2524e-02)
Test: [470/625]	Time  0.055 ( 0.029)	Loss 2.7170e-02 (3.2570e-02)
Test: [480/625]	Time  0.010 ( 0.028)	Loss 3.2769e-02 (3.2591e-02)
Test: [490/625]	Time  0.016 ( 0.028)	Loss 3.0145e-02 (3.2569e-02)
Test: [500/625]	Time  0.010 ( 0.028)	Loss 2.8626e-02 (3.2428e-02)
Test: [510/625]	Time  0.010 ( 0.028)	Loss 2.4641e-02 (3.2332e-02)
Test: [520/625]	Time  0.059 ( 0.028)	Loss 5.1959e-02 (3.2276e-02)
Test: [530/625]	Time  0.010 ( 0.028)	Loss 2.4498e-02 (3.2209e-02)
Test: [540/625]	Time  0.010 ( 0.028)	Loss 2.9351e-02 (3.2169e-02)
Test: [550/625]	Time  0.010 ( 0.028)	Loss 5.4886e-02 (3.2146e-02)
Test: [560/625]	Time  0.010 ( 0.027)	Loss 2.2324e-02 (3.2194e-02)
Test: [570/625]	Time  0.010 ( 0.027)	Loss 2.3455e-02 (3.2122e-02)
Test: [580/625]	Time  0.010 ( 0.027)	Loss 4.7592e-02 (3.2185e-02)
Test: [590/625]	Time  0.010 ( 0.027)	Loss 3.6842e-02 (3.2177e-02)
Test: [600/625]	Time  0.010 ( 0.026)	Loss 1.5109e-02 (3.2024e-02)
Test: [610/625]	Time  0.010 ( 0.026)	Loss 5.5020e-02 (3.1977e-02)
Test: [620/625]	Time  0.010 ( 0.026)	Loss 2.0853e-02 (3.1989e-02)
args.start_epoch =  0
Traceback (most recent call last):
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 748, in <module>
    main()
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 173, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 393, in main_worker
    old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2 = train(train_loader, model, criterion, optimizer, epoch, args, memory_loader,val_loader, model_without_ddp, old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2,r_model,c_model, r_c_loss_avg)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 491, in train
    metric_logger.update(loss=loss.item())
UnboundLocalError: local variable 'loss' referenced before assignment