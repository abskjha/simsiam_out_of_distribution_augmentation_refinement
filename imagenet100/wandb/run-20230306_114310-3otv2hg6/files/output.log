main_simsiam_imagenet100_ood_aug.py:157: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
| distributed init (rank 0): env://
ngpus_per_node 1
Use GPU: 0 for training
=> creating model 'resnet50'
DistributedDataParallel(
  (module): SimSiam(
    (encoder): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Sequential(
        (0): Linear(in_features=2048, out_features=2048, bias=False)
        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=2048, out_features=2048, bias=False)
        (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=2048, out_features=2048, bias=True)
        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (predictor): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=2048, bias=True)
    )
  )
)
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Length of train_dataset =  126689
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Epoch: [0][    0/15837]	Time  3.751 ( 3.751)	Data  2.457 ( 2.457)	Loss 2.3274e-02 (2.3274e-02)
Epoch: [0][   10/15837]	Time  0.078 ( 0.411)	Data  0.021 ( 0.241)	Loss -1.9533e-01 (-6.4087e-02)
Epoch: [0][   20/15837]	Time  0.079 ( 0.253)	Data  0.021 ( 0.136)	Loss -5.3928e-01 (-2.1850e-01)
Epoch: [0][   30/15837]	Time  0.079 ( 0.197)	Data  0.021 ( 0.099)	Loss -7.7590e-01 (-3.6820e-01)
Epoch: [0][   40/15837]	Time  0.080 ( 0.168)	Data  0.021 ( 0.080)	Loss -8.9181e-01 (-4.8502e-01)
Epoch: [0][   50/15837]	Time  0.079 ( 0.150)	Data  0.021 ( 0.068)	Loss -9.4420e-01 (-5.7127e-01)
Epoch: [0][   60/15837]	Time  0.079 ( 0.139)	Data  0.021 ( 0.061)	Loss -9.6970e-01 (-6.3514e-01)
Epoch: [0][   70/15837]	Time  0.078 ( 0.130)	Data  0.021 ( 0.055)	Loss -9.7580e-01 (-6.8295e-01)
Epoch: [0][   80/15837]	Time  0.079 ( 0.124)	Data  0.021 ( 0.051)	Loss -9.8358e-01 (-7.1994e-01)
Epoch: [0][   90/15837]	Time  0.078 ( 0.119)	Data  0.021 ( 0.048)	Loss -9.8631e-01 (-7.4931e-01)
Epoch: [0][  100/15837]	Time  0.078 ( 0.115)	Data  0.021 ( 0.045)	Loss -9.8959e-01 (-7.7290e-01)
Epoch: [0][  110/15837]	Time  0.078 ( 0.112)	Data  0.021 ( 0.043)	Loss -9.8881e-01 (-7.9243e-01)
Epoch: [0][  120/15837]	Time  0.079 ( 0.109)	Data  0.021 ( 0.041)	Loss -9.9030e-01 (-8.0877e-01)
Epoch: [0][  130/15837]	Time  0.079 ( 0.107)	Data  0.021 ( 0.039)	Loss -9.9114e-01 (-8.2263e-01)
Epoch: [0][  140/15837]	Time  0.079 ( 0.105)	Data  0.021 ( 0.038)	Loss -9.8931e-01 (-8.3450e-01)
Epoch: [0][  150/15837]	Time  0.079 ( 0.103)	Data  0.021 ( 0.037)	Loss -9.9266e-01 (-8.4482e-01)
Epoch: [0][  160/15837]	Time  0.079 ( 0.101)	Data  0.021 ( 0.036)	Loss -9.8864e-01 (-8.5387e-01)
Epoch: [0][  170/15837]	Time  0.079 ( 0.100)	Data  0.021 ( 0.035)	Loss -9.9152e-01 (-8.6184e-01)
Epoch: [0][  180/15837]	Time  0.078 ( 0.099)	Data  0.021 ( 0.034)	Loss -9.9012e-01 (-8.6897e-01)
Epoch: [0][  190/15837]	Time  0.078 ( 0.098)	Data  0.021 ( 0.034)	Loss -9.9258e-01 (-8.7533e-01)
Epoch: [0][  200/15837]	Time  0.078 ( 0.097)	Data  0.021 ( 0.033)	Loss -9.9158e-01 (-8.8110e-01)
Epoch: [0][  210/15837]	Time  0.079 ( 0.096)	Data  0.021 ( 0.032)	Loss -9.9009e-01 (-8.8633e-01)
Epoch: [0][  220/15837]	Time  0.079 ( 0.095)	Data  0.021 ( 0.032)	Loss -9.9025e-01 (-8.9108e-01)
Epoch: [0][  230/15837]	Time  0.079 ( 0.095)	Data  0.021 ( 0.031)	Loss -9.9333e-01 (-8.9546e-01)
Epoch: [0][  240/15837]	Time  0.080 ( 0.094)	Data  0.021 ( 0.031)	Loss -9.8942e-01 (-8.9944e-01)
Epoch: [0][  250/15837]	Time  0.079 ( 0.093)	Data  0.021 ( 0.031)	Loss -9.9146e-01 (-9.0311e-01)
Epoch: [0][  260/15837]	Time  0.078 ( 0.093)	Data  0.021 ( 0.030)	Loss -9.9122e-01 (-9.0649e-01)
Epoch: [0][  270/15837]	Time  0.079 ( 0.092)	Data  0.021 ( 0.030)	Loss -9.9309e-01 (-9.0964e-01)
Epoch: [0][  280/15837]	Time  0.078 ( 0.092)	Data  0.021 ( 0.030)	Loss -9.9262e-01 (-9.1258e-01)
Epoch: [0][  290/15837]	Time  0.078 ( 0.091)	Data  0.021 ( 0.029)	Loss -9.9366e-01 (-9.1532e-01)
Epoch: [0][  300/15837]	Time  0.078 ( 0.091)	Data  0.021 ( 0.029)	Loss -9.9230e-01 (-9.1787e-01)
Epoch: [0][  310/15837]	Time  0.079 ( 0.091)	Data  0.021 ( 0.029)	Loss -9.9098e-01 (-9.2024e-01)
Epoch: [0][  320/15837]	Time  0.079 ( 0.090)	Data  0.021 ( 0.029)	Loss -9.9316e-01 (-9.2251e-01)
Epoch: [0][  330/15837]	Time  0.077 ( 0.090)	Data  0.019 ( 0.028)	Loss -9.9296e-01 (-9.2464e-01)
Epoch: [0][  340/15837]	Time  0.079 ( 0.090)	Data  0.021 ( 0.028)	Loss -9.9130e-01 (-9.2665e-01)
Epoch: [0][  350/15837]	Time  0.079 ( 0.089)	Data  0.021 ( 0.028)	Loss -9.9314e-01 (-9.2856e-01)
Epoch: [0][  360/15837]	Time  0.079 ( 0.089)	Data  0.021 ( 0.028)	Loss -9.9343e-01 (-9.3036e-01)
Epoch: [0][  370/15837]	Time  0.079 ( 0.089)	Data  0.021 ( 0.028)	Loss -9.9318e-01 (-9.3205e-01)
Epoch: [0][  380/15837]	Time  0.078 ( 0.088)	Data  0.020 ( 0.027)	Loss -9.9333e-01 (-9.3365e-01)
Epoch: [0][  390/15837]	Time  0.079 ( 0.088)	Data  0.021 ( 0.027)	Loss -9.9249e-01 (-9.3518e-01)
Epoch: [0][  400/15837]	Time  0.078 ( 0.088)	Data  0.021 ( 0.027)	Loss -9.9326e-01 (-9.3662e-01)
Epoch: [0][  410/15837]	Time  0.079 ( 0.088)	Data  0.021 ( 0.027)	Loss -9.9398e-01 (-9.3801e-01)
Epoch: [0][  420/15837]	Time  0.079 ( 0.088)	Data  0.021 ( 0.027)	Loss -9.9387e-01 (-9.3934e-01)
Epoch: [0][  430/15837]	Time  0.079 ( 0.087)	Data  0.021 ( 0.027)	Loss -9.9387e-01 (-9.4059e-01)
Epoch: [0][  440/15837]	Time  0.079 ( 0.087)	Data  0.021 ( 0.026)	Loss -9.9368e-01 (-9.4180e-01)
Epoch: [0][  450/15837]	Time  0.079 ( 0.087)	Data  0.021 ( 0.026)	Loss -9.9273e-01 (-9.4295e-01)
Epoch: [0][  460/15837]	Time  0.080 ( 0.087)	Data  0.020 ( 0.026)	Loss -9.9432e-01 (-9.4405e-01)
Epoch: [0][  470/15837]	Time  0.079 ( 0.087)	Data  0.021 ( 0.026)	Loss -9.9531e-01 (-9.4512e-01)
Epoch: [0][  480/15837]	Time  0.079 ( 0.086)	Data  0.021 ( 0.026)	Loss -9.9361e-01 (-9.4614e-01)
Epoch: [0][  490/15837]	Time  0.078 ( 0.086)	Data  0.021 ( 0.026)	Loss -9.9430e-01 (-9.4712e-01)
Epoch: [0][  500/15837]	Time  0.079 ( 0.086)	Data  0.021 ( 0.026)	Loss -9.9465e-01 (-9.4806e-01)
Epoch: [0][  510/15837]	Time  0.079 ( 0.086)	Data  0.021 ( 0.026)	Loss -9.9483e-01 (-9.4897e-01)
Epoch: [0][  520/15837]	Time  0.079 ( 0.086)	Data  0.021 ( 0.026)	Loss -9.9512e-01 (-9.4984e-01)
Epoch: [0][  530/15837]	Time  0.080 ( 0.086)	Data  0.022 ( 0.026)	Loss -9.9512e-01 (-9.5068e-01)
Epoch: [0][  540/15837]	Time  0.078 ( 0.086)	Data  0.021 ( 0.025)	Loss -9.9452e-01 (-9.5150e-01)
Epoch: [0][  550/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9530e-01 (-9.5228e-01)
Epoch: [0][  560/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9340e-01 (-9.5304e-01)
Epoch: [0][  570/15837]	Time  0.078 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9412e-01 (-9.5377e-01)
Epoch: [0][  580/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9426e-01 (-9.5447e-01)
Epoch: [0][  590/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9538e-01 (-9.5516e-01)
Epoch: [0][  600/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9449e-01 (-9.5582e-01)
Epoch: [0][  610/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9384e-01 (-9.5646e-01)
Epoch: [0][  620/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9419e-01 (-9.5708e-01)
Epoch: [0][  630/15837]	Time  0.078 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9488e-01 (-9.5767e-01)
Epoch: [0][  640/15837]	Time  0.079 ( 0.085)	Data  0.021 ( 0.025)	Loss -9.9488e-01 (-9.5826e-01)
Epoch: [0][  650/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.025)	Loss -9.9506e-01 (-9.5882e-01)
Epoch: [0][  660/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.025)	Loss -9.9512e-01 (-9.5937e-01)
Epoch: [0][  670/15837]	Time  0.080 ( 0.084)	Data  0.021 ( 0.025)	Loss -9.9472e-01 (-9.5990e-01)
Epoch: [0][  680/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.025)	Loss -9.9576e-01 (-9.6043e-01)
Epoch: [0][  690/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.025)	Loss -9.9551e-01 (-9.6093e-01)
Epoch: [0][  700/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9607e-01 (-9.6143e-01)
Epoch: [0][  710/15837]	Time  0.080 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9584e-01 (-9.6191e-01)
Epoch: [0][  720/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9530e-01 (-9.6237e-01)
Epoch: [0][  730/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9584e-01 (-9.6283e-01)
Epoch: [0][  740/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9590e-01 (-9.6327e-01)
Epoch: [0][  750/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9613e-01 (-9.6370e-01)
Epoch: [0][  760/15837]	Time  0.079 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9573e-01 (-9.6413e-01)
Epoch: [0][  770/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9597e-01 (-9.6453e-01)
Epoch: [0][  780/15837]	Time  0.078 ( 0.084)	Data  0.021 ( 0.024)	Loss -9.9639e-01 (-9.6494e-01)
Epoch: [0][  790/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9588e-01 (-9.6532e-01)
Epoch: [0][  800/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9571e-01 (-9.6570e-01)
Epoch: [0][  810/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9495e-01 (-9.6607e-01)
Epoch: [0][  820/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9562e-01 (-9.6643e-01)
Epoch: [0][  830/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9527e-01 (-9.6677e-01)
Epoch: [0][  840/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9474e-01 (-9.6710e-01)
Epoch: [0][  850/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9508e-01 (-9.6743e-01)
Epoch: [0][  860/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9535e-01 (-9.6775e-01)
Epoch: [0][  870/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9524e-01 (-9.6806e-01)
Epoch: [0][  880/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9515e-01 (-9.6837e-01)
Epoch: [0][  890/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9515e-01 (-9.6867e-01)
Epoch: [0][  900/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9475e-01 (-9.6896e-01)
Epoch: [0][  910/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9520e-01 (-9.6925e-01)
Epoch: [0][  920/15837]	Time  0.080 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9539e-01 (-9.6953e-01)
Epoch: [0][  930/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9525e-01 (-9.6981e-01)
Epoch: [0][  940/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9546e-01 (-9.7008e-01)
Epoch: [0][  950/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9467e-01 (-9.7034e-01)
Epoch: [0][  960/15837]	Time  0.078 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9570e-01 (-9.7061e-01)
Epoch: [0][  970/15837]	Time  0.079 ( 0.083)	Data  0.021 ( 0.024)	Loss -9.9589e-01 (-9.7086e-01)
Epoch: [0][  980/15837]	Time  0.080 ( 0.083)	Data  0.019 ( 0.023)	Loss -9.9525e-01 (-9.7111e-01)
Epoch: [0][  990/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9581e-01 (-9.7135e-01)
Epoch: [0][ 1000/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9552e-01 (-9.7159e-01)
Epoch: [0][ 1010/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9508e-01 (-9.7182e-01)
Epoch: [0][ 1020/15837]	Time  0.080 ( 0.082)	Data  0.020 ( 0.023)	Loss -9.9572e-01 (-9.7206e-01)
Epoch: [0][ 1030/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9536e-01 (-9.7228e-01)
Epoch: [0][ 1040/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9552e-01 (-9.7251e-01)
Epoch: [0][ 1050/15837]	Time  0.078 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9556e-01 (-9.7273e-01)
Epoch: [0][ 1060/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9561e-01 (-9.7294e-01)
Epoch: [0][ 1070/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9456e-01 (-9.7315e-01)
Epoch: [0][ 1080/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9628e-01 (-9.7336e-01)
Epoch: [0][ 1090/15837]	Time  0.078 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9538e-01 (-9.7356e-01)
Epoch: [0][ 1100/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9594e-01 (-9.7376e-01)
Epoch: [0][ 1110/15837]	Time  0.078 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9556e-01 (-9.7396e-01)
Epoch: [0][ 1120/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9602e-01 (-9.7415e-01)
Epoch: [0][ 1130/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9586e-01 (-9.7434e-01)
Epoch: [0][ 1140/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9572e-01 (-9.7453e-01)
Epoch: [0][ 1150/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9499e-01 (-9.7471e-01)
Epoch: [0][ 1160/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9526e-01 (-9.7489e-01)
Epoch: [0][ 1170/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9574e-01 (-9.7506e-01)
Epoch: [0][ 1180/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9487e-01 (-9.7523e-01)
Epoch: [0][ 1190/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9523e-01 (-9.7540e-01)
Epoch: [0][ 1200/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9550e-01 (-9.7557e-01)
Epoch: [0][ 1210/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9473e-01 (-9.7573e-01)
Epoch: [0][ 1220/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9482e-01 (-9.7590e-01)
Epoch: [0][ 1230/15837]	Time  0.078 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9577e-01 (-9.7605e-01)
Epoch: [0][ 1240/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9524e-01 (-9.7621e-01)
Epoch: [0][ 1250/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9602e-01 (-9.7637e-01)
Epoch: [0][ 1260/15837]	Time  0.080 ( 0.082)	Data  0.022 ( 0.023)	Loss -9.9534e-01 (-9.7652e-01)
Epoch: [0][ 1270/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9491e-01 (-9.7667e-01)
Epoch: [0][ 1280/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9566e-01 (-9.7682e-01)
Epoch: [0][ 1290/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9592e-01 (-9.7697e-01)
Epoch: [0][ 1300/15837]	Time  0.080 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9592e-01 (-9.7711e-01)
Epoch: [0][ 1310/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9576e-01 (-9.7725e-01)
Epoch: [0][ 1320/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9593e-01 (-9.7739e-01)
Epoch: [0][ 1330/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9608e-01 (-9.7753e-01)
Epoch: [0][ 1340/15837]	Time  0.079 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9598e-01 (-9.7767e-01)
Epoch: [0][ 1350/15837]	Time  0.078 ( 0.082)	Data  0.021 ( 0.023)	Loss -9.9608e-01 (-9.7780e-01)
Epoch: [0][ 1360/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9544e-01 (-9.7794e-01)
Epoch: [0][ 1370/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9577e-01 (-9.7807e-01)
Epoch: [0][ 1380/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9587e-01 (-9.7820e-01)
Epoch: [0][ 1390/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9615e-01 (-9.7832e-01)
Epoch: [0][ 1400/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9626e-01 (-9.7845e-01)
Epoch: [0][ 1410/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9547e-01 (-9.7857e-01)
Epoch: [0][ 1420/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9540e-01 (-9.7869e-01)
Epoch: [0][ 1430/15837]	Time  0.080 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9625e-01 (-9.7881e-01)
Epoch: [0][ 1440/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9633e-01 (-9.7894e-01)
Epoch: [0][ 1450/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9648e-01 (-9.7905e-01)
Epoch: [0][ 1460/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9581e-01 (-9.7917e-01)
Epoch: [0][ 1470/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9638e-01 (-9.7928e-01)
Epoch: [0][ 1480/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9613e-01 (-9.7940e-01)
Epoch: [0][ 1490/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9636e-01 (-9.7951e-01)
Epoch: [0][ 1500/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9577e-01 (-9.7962e-01)
Epoch: [0][ 1510/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9648e-01 (-9.7972e-01)
Epoch: [0][ 1520/15837]	Time  0.080 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9644e-01 (-9.7983e-01)
Epoch: [0][ 1530/15837]	Time  0.080 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9604e-01 (-9.7994e-01)
Epoch: [0][ 1540/15837]	Time  0.078 ( 0.081)	Data  0.020 ( 0.023)	Loss -9.9592e-01 (-9.8004e-01)
Epoch: [0][ 1550/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9597e-01 (-9.8015e-01)
Epoch: [0][ 1560/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9597e-01 (-9.8025e-01)
Epoch: [0][ 1570/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9604e-01 (-9.8035e-01)
Epoch: [0][ 1580/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9639e-01 (-9.8045e-01)
Epoch: [0][ 1590/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9613e-01 (-9.8055e-01)
Epoch: [0][ 1600/15837]	Time  0.080 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9565e-01 (-9.8064e-01)
Epoch: [0][ 1610/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9619e-01 (-9.8074e-01)
Epoch: [0][ 1620/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9613e-01 (-9.8084e-01)
Epoch: [0][ 1630/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.023)	Loss -9.9660e-01 (-9.8093e-01)
Epoch: [0][ 1640/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9655e-01 (-9.8103e-01)
Epoch: [0][ 1650/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9627e-01 (-9.8112e-01)
Epoch: [0][ 1660/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9666e-01 (-9.8122e-01)
Epoch: [0][ 1670/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9669e-01 (-9.8131e-01)
Epoch: [0][ 1680/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9704e-01 (-9.8140e-01)
Epoch: [0][ 1690/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9690e-01 (-9.8149e-01)
Epoch: [0][ 1700/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9676e-01 (-9.8158e-01)
Epoch: [0][ 1710/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9680e-01 (-9.8167e-01)
Epoch: [0][ 1720/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9692e-01 (-9.8176e-01)
Epoch: [0][ 1730/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9697e-01 (-9.8184e-01)
Epoch: [0][ 1740/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9692e-01 (-9.8193e-01)
Epoch: [0][ 1750/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9665e-01 (-9.8201e-01)
Epoch: [0][ 1760/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9686e-01 (-9.8210e-01)
Epoch: [0][ 1770/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9642e-01 (-9.8218e-01)
Epoch: [0][ 1780/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9637e-01 (-9.8226e-01)
Epoch: [0][ 1790/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9671e-01 (-9.8234e-01)
Epoch: [0][ 1800/15837]	Time  0.078 ( 0.081)	Data  0.020 ( 0.022)	Loss -9.9665e-01 (-9.8242e-01)
Epoch: [0][ 1810/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9611e-01 (-9.8249e-01)
Epoch: [0][ 1820/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9638e-01 (-9.8257e-01)
Epoch: [0][ 1830/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9657e-01 (-9.8265e-01)
Epoch: [0][ 1840/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9674e-01 (-9.8272e-01)
Epoch: [0][ 1850/15837]	Time  0.080 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9664e-01 (-9.8280e-01)
Epoch: [0][ 1860/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9640e-01 (-9.8287e-01)
Epoch: [0][ 1870/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9687e-01 (-9.8295e-01)
Epoch: [0][ 1880/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9649e-01 (-9.8302e-01)
Epoch: [0][ 1890/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9668e-01 (-9.8309e-01)
Epoch: [0][ 1900/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9678e-01 (-9.8316e-01)
Epoch: [0][ 1910/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9666e-01 (-9.8323e-01)
Epoch: [0][ 1920/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9661e-01 (-9.8330e-01)
Epoch: [0][ 1930/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9647e-01 (-9.8337e-01)
Epoch: [0][ 1940/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9663e-01 (-9.8344e-01)
Epoch: [0][ 1950/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9691e-01 (-9.8351e-01)
Epoch: [0][ 1960/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9704e-01 (-9.8358e-01)
Epoch: [0][ 1970/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9687e-01 (-9.8364e-01)
Epoch: [0][ 1980/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9675e-01 (-9.8371e-01)
Epoch: [0][ 1990/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9691e-01 (-9.8378e-01)
Epoch: [0][ 2000/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9682e-01 (-9.8384e-01)
Epoch: [0][ 2010/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9705e-01 (-9.8391e-01)
Epoch: [0][ 2020/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9722e-01 (-9.8397e-01)
Epoch: [0][ 2030/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9679e-01 (-9.8403e-01)
Epoch: [0][ 2040/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9686e-01 (-9.8410e-01)
Epoch: [0][ 2050/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9688e-01 (-9.8416e-01)
Epoch: [0][ 2060/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9702e-01 (-9.8422e-01)
Epoch: [0][ 2070/15837]	Time  0.078 ( 0.081)	Data  0.020 ( 0.022)	Loss -9.9678e-01 (-9.8428e-01)
Epoch: [0][ 2080/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9655e-01 (-9.8434e-01)
Epoch: [0][ 2090/15837]	Time  0.079 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9663e-01 (-9.8440e-01)
Epoch: [0][ 2100/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9619e-01 (-9.8446e-01)
Epoch: [0][ 2110/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9661e-01 (-9.8452e-01)
Epoch: [0][ 2120/15837]	Time  0.078 ( 0.081)	Data  0.021 ( 0.022)	Loss -9.9669e-01 (-9.8457e-01)
Epoch: [0][ 2130/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9642e-01 (-9.8463e-01)
Epoch: [0][ 2140/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9664e-01 (-9.8469e-01)
Epoch: [0][ 2150/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9666e-01 (-9.8474e-01)
Epoch: [0][ 2160/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9647e-01 (-9.8479e-01)
Epoch: [0][ 2170/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9670e-01 (-9.8485e-01)
Epoch: [0][ 2180/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9628e-01 (-9.8490e-01)
Epoch: [0][ 2190/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9659e-01 (-9.8496e-01)
Epoch: [0][ 2200/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9676e-01 (-9.8501e-01)
Epoch: [0][ 2210/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9647e-01 (-9.8506e-01)
Epoch: [0][ 2220/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9684e-01 (-9.8511e-01)
Epoch: [0][ 2230/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9696e-01 (-9.8517e-01)
Epoch: [0][ 2240/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9655e-01 (-9.8522e-01)
Epoch: [0][ 2250/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9645e-01 (-9.8527e-01)
Epoch: [0][ 2260/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9644e-01 (-9.8532e-01)
Epoch: [0][ 2270/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9609e-01 (-9.8537e-01)
Epoch: [0][ 2280/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9667e-01 (-9.8542e-01)
Epoch: [0][ 2290/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9642e-01 (-9.8547e-01)
Epoch: [0][ 2300/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9674e-01 (-9.8551e-01)
Epoch: [0][ 2310/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9680e-01 (-9.8556e-01)
Epoch: [0][ 2320/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9714e-01 (-9.8561e-01)
Epoch: [0][ 2330/15837]	Time  0.078 ( 0.080)	Data  0.018 ( 0.022)	Loss -9.9654e-01 (-9.8566e-01)
Epoch: [0][ 2340/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9695e-01 (-9.8571e-01)
Epoch: [0][ 2350/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9665e-01 (-9.8575e-01)
Epoch: [0][ 2360/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9637e-01 (-9.8580e-01)
Epoch: [0][ 2370/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9670e-01 (-9.8584e-01)
Epoch: [0][ 2380/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9642e-01 (-9.8589e-01)
Epoch: [0][ 2390/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9692e-01 (-9.8593e-01)
Epoch: [0][ 2400/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9684e-01 (-9.8598e-01)
Epoch: [0][ 2410/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9690e-01 (-9.8602e-01)
Epoch: [0][ 2420/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9660e-01 (-9.8607e-01)
Epoch: [0][ 2430/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9711e-01 (-9.8611e-01)
Epoch: [0][ 2440/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9691e-01 (-9.8616e-01)
Epoch: [0][ 2450/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9669e-01 (-9.8620e-01)
Epoch: [0][ 2460/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9721e-01 (-9.8625e-01)
Epoch: [0][ 2470/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9673e-01 (-9.8629e-01)
Epoch: [0][ 2480/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9700e-01 (-9.8633e-01)
Epoch: [0][ 2490/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9683e-01 (-9.8637e-01)
Epoch: [0][ 2500/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9676e-01 (-9.8642e-01)
Epoch: [0][ 2510/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9686e-01 (-9.8646e-01)
Epoch: [0][ 2520/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9719e-01 (-9.8650e-01)
Epoch: [0][ 2530/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9678e-01 (-9.8654e-01)
Epoch: [0][ 2540/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9704e-01 (-9.8658e-01)
Epoch: [0][ 2550/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9720e-01 (-9.8662e-01)
Epoch: [0][ 2560/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9642e-01 (-9.8666e-01)
Epoch: [0][ 2570/15837]	Time  0.078 ( 0.080)	Data  0.020 ( 0.022)	Loss -9.9704e-01 (-9.8670e-01)
Epoch: [0][ 2580/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9700e-01 (-9.8674e-01)
Epoch: [0][ 2590/15837]	Time  0.077 ( 0.080)	Data  0.019 ( 0.022)	Loss -9.9712e-01 (-9.8678e-01)
Epoch: [0][ 2600/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9697e-01 (-9.8682e-01)
Epoch: [0][ 2610/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9714e-01 (-9.8686e-01)
Epoch: [0][ 2620/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9706e-01 (-9.8690e-01)
Epoch: [0][ 2630/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9717e-01 (-9.8694e-01)
Epoch: [0][ 2640/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9702e-01 (-9.8697e-01)
Epoch: [0][ 2650/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9726e-01 (-9.8701e-01)
Epoch: [0][ 2660/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9696e-01 (-9.8705e-01)
Epoch: [0][ 2670/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9687e-01 (-9.8709e-01)
Epoch: [0][ 2680/15837]	Time  0.077 ( 0.080)	Data  0.020 ( 0.022)	Loss -9.9713e-01 (-9.8713e-01)
Epoch: [0][ 2690/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9713e-01 (-9.8716e-01)
Epoch: [0][ 2700/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9708e-01 (-9.8720e-01)
Epoch: [0][ 2710/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9708e-01 (-9.8724e-01)
Epoch: [0][ 2720/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9714e-01 (-9.8727e-01)
Epoch: [0][ 2730/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9578e-01 (-9.8731e-01)
Epoch: [0][ 2740/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9684e-01 (-9.8734e-01)
Epoch: [0][ 2750/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9639e-01 (-9.8738e-01)
Epoch: [0][ 2760/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9710e-01 (-9.8741e-01)
Epoch: [0][ 2770/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9669e-01 (-9.8745e-01)
Epoch: [0][ 2780/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9681e-01 (-9.8748e-01)
Epoch: [0][ 2790/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9708e-01 (-9.8751e-01)
Epoch: [0][ 2800/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9676e-01 (-9.8755e-01)
Epoch: [0][ 2810/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9673e-01 (-9.8758e-01)
Epoch: [0][ 2820/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9686e-01 (-9.8761e-01)
Epoch: [0][ 2830/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9674e-01 (-9.8764e-01)
Epoch: [0][ 2840/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9688e-01 (-9.8768e-01)
Epoch: [0][ 2850/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9698e-01 (-9.8771e-01)
Epoch: [0][ 2860/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9668e-01 (-9.8774e-01)
Epoch: [0][ 2870/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9697e-01 (-9.8777e-01)
Epoch: [0][ 2880/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9688e-01 (-9.8781e-01)
Epoch: [0][ 2890/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9702e-01 (-9.8784e-01)
Epoch: [0][ 2900/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9711e-01 (-9.8787e-01)
Epoch: [0][ 2910/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9681e-01 (-9.8790e-01)
Epoch: [0][ 2920/15837]	Time  0.078 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9717e-01 (-9.8793e-01)
Epoch: [0][ 2930/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9706e-01 (-9.8796e-01)
Epoch: [0][ 2940/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9693e-01 (-9.8799e-01)
Epoch: [0][ 2950/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9742e-01 (-9.8802e-01)
Epoch: [0][ 2960/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9709e-01 (-9.8805e-01)
Epoch: [0][ 2970/15837]	Time  0.079 ( 0.080)	Data  0.021 ( 0.022)	Loss -9.9709e-01 (-9.8808e-01)
Traceback (most recent call last):
  File "main_simsiam_imagenet100_ood_aug.py", line 675, in <module>
    main()
  File "main_simsiam_imagenet100_ood_aug.py", line 168, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "main_simsiam_imagenet100_ood_aug.py", line 355, in main_worker
    ood_train(memory_loader, c_model, r_model, c_model_optimizer, epoch, args)
  File "main_simsiam_imagenet100_ood_aug.py", line 645, in ood_train
    r_out = r_model(images)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 220, in forward
    return self._forward_impl(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 211, in _forward_impl
    x = self.layer4(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 108, in forward
    out = self.conv2(out)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 423, in forward
    return self._conv_forward(input, self.weight)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 420, in _conv_forward
    self.padding, self.dilation, self.groups)
KeyboardInterrupt