| distributed init (rank 0): env://
ngpus_per_node 1
Use GPU: 0 for training
=> creating model 'resnet50'
main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py:162: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
DistributedDataParallel(
  (module): SimSiam(
    (encoder): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Sequential(
        (0): Linear(in_features=2048, out_features=2048, bias=False)
        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=2048, out_features=2048, bias=False)
        (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=2048, out_features=2048, bias=True)
        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (predictor): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=2048, bias=True)
    )
  )
)
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Length of train_dataset =  126689
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Epoch: [0][  0/625]	Time  3.310 ( 3.310)	Data  2.522 ( 2.522)	Loss 2.3758e+00 (2.3758e+00)
Epoch: [0][ 10/625]	Time  0.027 ( 0.324)	Data  0.007 ( 0.235)	Loss 1.8991e+00 (2.3330e+00)
Epoch: [0][ 20/625]	Time  0.025 ( 0.182)	Data  0.006 ( 0.126)	Loss 2.2706e+00 (2.0913e+00)
Epoch: [0][ 30/625]	Time  0.026 ( 0.132)	Data  0.007 ( 0.088)	Loss 2.0614e+00 (2.0683e+00)
Epoch: [0][ 40/625]	Time  0.026 ( 0.106)	Data  0.007 ( 0.068)	Loss 1.2465e+00 (1.9145e+00)
Epoch: [0][ 50/625]	Time  0.069 ( 0.092)	Data  0.007 ( 0.056)	Loss 1.1783e+00 (1.8609e+00)
Epoch: [0][ 60/625]	Time  0.020 ( 0.084)	Data  0.001 ( 0.047)	Loss 7.3688e-01 (1.7591e+00)
Epoch: [0][ 70/625]	Time  0.032 ( 0.076)	Data  0.007 ( 0.042)	Loss 9.3344e-01 (1.6972e+00)
Epoch: [0][ 80/625]	Time  0.028 ( 0.070)	Data  0.007 ( 0.037)	Loss 1.2354e+00 (1.6267e+00)
Epoch: [0][ 90/625]	Time  0.026 ( 0.065)	Data  0.007 ( 0.034)	Loss 8.7761e-01 (1.5525e+00)
Epoch: [0][100/625]	Time  0.026 ( 0.061)	Data  0.007 ( 0.031)	Loss 8.9883e-01 (1.4701e+00)
Epoch: [0][110/625]	Time  0.026 ( 0.058)	Data  0.007 ( 0.029)	Loss 7.1377e-01 (1.4049e+00)
Epoch: [0][120/625]	Time  0.026 ( 0.055)	Data  0.006 ( 0.027)	Loss 6.7253e-01 (1.3499e+00)
Epoch: [0][130/625]	Time  0.025 ( 0.053)	Data  0.006 ( 0.026)	Loss 2.7131e-01 (1.2897e+00)
Epoch: [0][140/625]	Time  0.025 ( 0.051)	Data  0.006 ( 0.024)	Loss 5.5916e-01 (1.2335e+00)
Epoch: [0][150/625]	Time  0.026 ( 0.050)	Data  0.007 ( 0.023)	Loss 3.2576e-01 (1.1754e+00)
Epoch: [0][160/625]	Time  0.024 ( 0.048)	Data  0.006 ( 0.022)	Loss 4.1810e-01 (1.1195e+00)
Epoch: [0][170/625]	Time  0.026 ( 0.047)	Data  0.006 ( 0.021)	Loss 6.5731e-01 (1.0723e+00)
Epoch: [0][180/625]	Time  0.026 ( 0.046)	Data  0.007 ( 0.020)	Loss 2.7098e-01 (1.0226e+00)
Epoch: [0][190/625]	Time  0.026 ( 0.045)	Data  0.007 ( 0.020)	Loss 3.7932e-01 (9.8790e-01)
Epoch: [0][200/625]	Time  0.026 ( 0.044)	Data  0.007 ( 0.019)	Loss 1.4346e-01 (9.5179e-01)
Epoch: [0][210/625]	Time  0.025 ( 0.043)	Data  0.006 ( 0.018)	Loss 1.0593e-01 (9.1795e-01)
Epoch: [0][220/625]	Time  0.025 ( 0.042)	Data  0.007 ( 0.018)	Loss 3.0719e-01 (8.8420e-01)
Epoch: [0][230/625]	Time  0.025 ( 0.041)	Data  0.007 ( 0.017)	Loss 2.5925e-01 (8.5157e-01)
Epoch: [0][240/625]	Time  0.025 ( 0.041)	Data  0.006 ( 0.017)	Loss 2.2117e-01 (8.2476e-01)
Epoch: [0][250/625]	Time  0.027 ( 0.040)	Data  0.007 ( 0.017)	Loss 1.4536e-01 (7.9659e-01)
Epoch: [0][260/625]	Time  0.028 ( 0.040)	Data  0.007 ( 0.016)	Loss 1.2819e-01 (7.7278e-01)
Epoch: [0][270/625]	Time  0.034 ( 0.039)	Data  0.006 ( 0.016)	Loss 6.6861e-02 (7.4612e-01)
Epoch: [0][280/625]	Time  0.026 ( 0.039)	Data  0.007 ( 0.015)	Loss 1.9828e-01 (7.2418e-01)
Epoch: [0][290/625]	Time  0.026 ( 0.038)	Data  0.007 ( 0.015)	Loss 5.8615e-02 (7.0256e-01)
Epoch: [0][300/625]	Time  0.026 ( 0.038)	Data  0.007 ( 0.015)	Loss 1.8937e-01 (6.8359e-01)
Epoch: [0][310/625]	Time  0.028 ( 0.038)	Data  0.007 ( 0.015)	Loss 1.6376e-01 (6.6871e-01)
Epoch: [0][320/625]	Time  0.026 ( 0.037)	Data  0.007 ( 0.014)	Loss 4.6620e-02 (6.5069e-01)
Epoch: [0][330/625]	Time  0.027 ( 0.037)	Data  0.007 ( 0.014)	Loss 1.7413e-01 (6.3424e-01)
Epoch: [0][340/625]	Time  0.027 ( 0.037)	Data  0.007 ( 0.014)	Loss 6.6797e-02 (6.1827e-01)
Epoch: [0][350/625]	Time  0.025 ( 0.036)	Data  0.007 ( 0.014)	Loss 4.3676e-02 (6.0250e-01)
Epoch: [0][360/625]	Time  0.025 ( 0.036)	Data  0.006 ( 0.014)	Loss 3.7365e-02 (5.8744e-01)
Epoch: [0][370/625]	Time  0.026 ( 0.036)	Data  0.007 ( 0.013)	Loss 1.6961e-01 (5.7312e-01)
Epoch: [0][380/625]	Time  0.052 ( 0.036)	Data  0.007 ( 0.013)	Loss 5.0914e-02 (5.5933e-01)
Epoch: [0][390/625]	Time  0.025 ( 0.036)	Data  0.007 ( 0.013)	Loss 2.4454e-02 (5.4710e-01)
Epoch: [0][400/625]	Time  0.028 ( 0.036)	Data  0.007 ( 0.013)	Loss 5.3171e-02 (5.3481e-01)
Epoch: [0][410/625]	Time  0.028 ( 0.036)	Data  0.006 ( 0.013)	Loss 1.1482e-01 (5.2304e-01)
Epoch: [0][420/625]	Time  0.023 ( 0.035)	Data  0.001 ( 0.013)	Loss 1.6701e-01 (5.1221e-01)
Epoch: [0][430/625]	Time  0.027 ( 0.035)	Data  0.007 ( 0.012)	Loss 2.6562e-02 (5.0246e-01)
Epoch: [0][440/625]	Time  0.051 ( 0.035)	Data  0.030 ( 0.012)	Loss 3.3047e-02 (4.9189e-01)
Epoch: [0][450/625]	Time  0.026 ( 0.035)	Data  0.006 ( 0.012)	Loss 7.7300e-02 (4.8419e-01)
Epoch: [0][460/625]	Time  0.026 ( 0.035)	Data  0.007 ( 0.012)	Loss 6.9423e-02 (4.7459e-01)
Epoch: [0][470/625]	Time  0.026 ( 0.035)	Data  0.007 ( 0.012)	Loss 4.2511e-02 (4.6562e-01)
Epoch: [0][480/625]	Time  0.026 ( 0.035)	Data  0.007 ( 0.012)	Loss 3.6451e-02 (4.5736e-01)
Epoch: [0][490/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.012)	Loss 3.4946e-02 (4.4949e-01)
Epoch: [0][500/625]	Time  0.038 ( 0.034)	Data  0.005 ( 0.012)	Loss 6.4705e-02 (4.4153e-01)
Epoch: [0][510/625]	Time  0.026 ( 0.034)	Data  0.005 ( 0.011)	Loss 3.3329e-02 (4.3388e-01)
Epoch: [0][520/625]	Time  0.030 ( 0.034)	Data  0.006 ( 0.011)	Loss 8.9439e-02 (4.2634e-01)
Epoch: [0][530/625]	Time  0.026 ( 0.034)	Data  0.005 ( 0.011)	Loss 2.6425e-02 (4.1918e-01)
Epoch: [0][540/625]	Time  0.025 ( 0.034)	Data  0.007 ( 0.011)	Loss 8.7577e-02 (4.1238e-01)
Epoch: [0][550/625]	Time  0.027 ( 0.034)	Data  0.007 ( 0.011)	Loss 2.4199e-01 (4.0666e-01)
Epoch: [0][560/625]	Time  0.025 ( 0.034)	Data  0.006 ( 0.011)	Loss 2.3515e-02 (4.0113e-01)
Epoch: [0][570/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 5.8890e-02 (3.9489e-01)
Epoch: [0][580/625]	Time  0.025 ( 0.033)	Data  0.006 ( 0.011)	Loss 1.5779e-01 (3.8944e-01)
Epoch: [0][590/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 5.8037e-02 (3.8407e-01)
Epoch: [0][600/625]	Time  0.026 ( 0.033)	Data  0.007 ( 0.011)	Loss 2.9917e-02 (3.7815e-01)
Epoch: [0][610/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.9091e-01 (3.7283e-01)
Epoch: [0][620/625]	Time  0.025 ( 0.033)	Data  0.006 ( 0.011)	Loss 1.9085e-02 (3.6755e-01)
Test: [  0/625]	Time  2.706 ( 2.706)	Loss 3.7394e-02 (3.7394e-02)
Test: [ 10/625]	Time  0.010 ( 0.265)	Loss 4.1214e-02 (4.1563e-02)
Test: [ 20/625]	Time  0.011 ( 0.153)	Loss 2.3183e-02 (3.4483e-02)
Test: [ 30/625]	Time  0.026 ( 0.114)	Loss 2.6149e-02 (3.2391e-02)
Test: [ 40/625]	Time  0.068 ( 0.092)	Loss 1.9181e-02 (3.0051e-02)
Test: [ 50/625]	Time  0.010 ( 0.079)	Loss 1.9244e-02 (3.3444e-02)
Test: [ 60/625]	Time  0.010 ( 0.072)	Loss 3.4653e-02 (3.4220e-02)
Test: [ 70/625]	Time  0.010 ( 0.066)	Loss 3.1751e-02 (3.5413e-02)
Test: [ 80/625]	Time  0.010 ( 0.062)	Loss 4.1920e-02 (3.5845e-02)
Test: [ 90/625]	Time  0.010 ( 0.057)	Loss 4.6220e-02 (3.6300e-02)
Test: [100/625]	Time  0.015 ( 0.054)	Loss 3.2940e-02 (3.5530e-02)
Test: [110/625]	Time  0.010 ( 0.051)	Loss 2.9986e-02 (3.5455e-02)
Test: [120/625]	Time  0.011 ( 0.049)	Loss 4.9144e-02 (3.6417e-02)
Test: [130/625]	Time  0.010 ( 0.047)	Loss 2.1421e-02 (3.5987e-02)
Test: [140/625]	Time  0.010 ( 0.045)	Loss 2.7713e-02 (3.5600e-02)
Test: [150/625]	Time  0.073 ( 0.044)	Loss 3.0593e-02 (3.5386e-02)
Test: [160/625]	Time  0.011 ( 0.042)	Loss 5.0681e-02 (3.5129e-02)
Test: [170/625]	Time  0.010 ( 0.041)	Loss 7.2133e-02 (3.5488e-02)
Test: [180/625]	Time  0.010 ( 0.040)	Loss 2.1546e-02 (3.5262e-02)
Test: [190/625]	Time  0.010 ( 0.039)	Loss 6.0354e-02 (3.5265e-02)
Test: [200/625]	Time  0.059 ( 0.038)	Loss 4.9244e-02 (3.5594e-02)
Test: [210/625]	Time  0.010 ( 0.037)	Loss 1.9673e-02 (3.5751e-02)
Test: [220/625]	Time  0.009 ( 0.037)	Loss 4.2562e-02 (3.5404e-02)
Test: [230/625]	Time  0.010 ( 0.036)	Loss 3.3856e-02 (3.5217e-02)
Test: [240/625]	Time  0.010 ( 0.035)	Loss 2.3851e-02 (3.5187e-02)
Test: [250/625]	Time  0.009 ( 0.035)	Loss 2.0647e-02 (3.4992e-02)
Test: [260/625]	Time  0.009 ( 0.034)	Loss 4.4351e-02 (3.5098e-02)
Test: [270/625]	Time  0.010 ( 0.034)	Loss 2.6882e-02 (3.4982e-02)
Test: [280/625]	Time  0.010 ( 0.034)	Loss 4.0292e-02 (3.5002e-02)
Test: [290/625]	Time  0.011 ( 0.033)	Loss 3.6162e-02 (3.5031e-02)
Test: [300/625]	Time  0.009 ( 0.033)	Loss 4.9184e-02 (3.5177e-02)
Test: [310/625]	Time  0.011 ( 0.033)	Loss 2.9022e-02 (3.5178e-02)
Test: [320/625]	Time  0.010 ( 0.032)	Loss 3.3757e-02 (3.5463e-02)
Test: [330/625]	Time  0.011 ( 0.032)	Loss 7.4399e-02 (3.5740e-02)
Test: [340/625]	Time  0.010 ( 0.032)	Loss 2.7452e-02 (3.5808e-02)
Test: [350/625]	Time  0.010 ( 0.031)	Loss 2.7875e-02 (3.5782e-02)
Test: [360/625]	Time  0.010 ( 0.031)	Loss 3.1958e-02 (3.5833e-02)
Test: [370/625]	Time  0.009 ( 0.031)	Loss 5.9732e-02 (3.5657e-02)
Test: [380/625]	Time  0.009 ( 0.030)	Loss 2.6872e-02 (3.5613e-02)
Test: [390/625]	Time  0.074 ( 0.031)	Loss 2.1395e-02 (3.5981e-02)
Test: [400/625]	Time  0.069 ( 0.030)	Loss 2.5996e-02 (3.5815e-02)
Test: [410/625]	Time  0.010 ( 0.030)	Loss 4.5607e-02 (3.5737e-02)
Test: [420/625]	Time  0.009 ( 0.030)	Loss 5.7331e-02 (3.5689e-02)
Test: [430/625]	Time  0.079 ( 0.030)	Loss 3.1640e-02 (3.5824e-02)
Test: [440/625]	Time  0.010 ( 0.030)	Loss 3.6468e-02 (3.5703e-02)
Test: [450/625]	Time  0.009 ( 0.030)	Loss 3.0734e-02 (3.6175e-02)
Test: [460/625]	Time  0.010 ( 0.030)	Loss 2.3352e-02 (3.5960e-02)
Test: [470/625]	Time  0.010 ( 0.029)	Loss 3.1245e-02 (3.6053e-02)
Test: [480/625]	Time  0.010 ( 0.029)	Loss 3.6957e-02 (3.6114e-02)
Test: [490/625]	Time  0.009 ( 0.029)	Loss 3.1673e-02 (3.6123e-02)
Test: [500/625]	Time  0.061 ( 0.029)	Loss 2.9347e-02 (3.5952e-02)
Test: [510/625]	Time  0.011 ( 0.029)	Loss 2.6149e-02 (3.5823e-02)
Test: [520/625]	Time  0.066 ( 0.029)	Loss 5.6804e-02 (3.5746e-02)
Test: [530/625]	Time  0.010 ( 0.028)	Loss 2.5800e-02 (3.5729e-02)
Test: [540/625]	Time  0.010 ( 0.028)	Loss 3.4808e-02 (3.5654e-02)
Test: [550/625]	Time  0.068 ( 0.028)	Loss 8.6960e-02 (3.5667e-02)
Test: [560/625]	Time  0.009 ( 0.028)	Loss 2.0255e-02 (3.5823e-02)
Test: [570/625]	Time  0.009 ( 0.028)	Loss 2.6146e-02 (3.5793e-02)
Test: [580/625]	Time  0.009 ( 0.028)	Loss 7.2865e-02 (3.6003e-02)
Test: [590/625]	Time  0.009 ( 0.027)	Loss 3.7201e-02 (3.6082e-02)
Test: [600/625]	Time  0.009 ( 0.027)	Loss 1.7318e-02 (3.5873e-02)
Test: [610/625]	Time  0.009 ( 0.027)	Loss 7.2842e-02 (3.5834e-02)
Test: [620/625]	Time  0.009 ( 0.026)	Loss 2.0071e-02 (3.5894e-02)
args.start_epoch =  0
Epoch: [0][    0/15836]	Time  5.500 ( 5.500)	Data  4.544 ( 4.544)	Loss 0.0062 (0.0062)
Epoch: [0/1000]  [    0/15836]  eta: 1 day, 0:11:49  loss: 0.006242 (0.006242)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 5.500747  data: 4.543561  max mem: 3283
Traceback (most recent call last):
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 747, in <module>
    main()
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 173, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 393, in main_worker
    old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2 = train(train_loader, model, criterion, optimizer, epoch, args, memory_loader,val_loader, model_without_ddp, old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2,r_model,c_model, r_c_loss_avg)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 469, in train
    p1, p2, z1, z2 = model(x1=images_0, x2=images_1)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ".././simsiam/builder.py", line 55, in forward
    z1 = self.encoder(x1) # NxC
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 220, in forward
    return self._forward_impl(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 211, in _forward_impl
    x = self.layer4(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 104, in forward
    out = self.conv1(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 423, in forward
    return self._conv_forward(input, self.weight)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 420, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 2.26 GiB (GPU 0; 23.87 GiB total capacity; 954.62 MiB already allocated; 21.47 GiB free; 1.70 GiB reserved in total by PyTorch)