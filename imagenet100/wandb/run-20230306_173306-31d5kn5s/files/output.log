main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py:162: UserWarning: You have chosen a specific GPU. This will completely disable data parallelism.
  warnings.warn('You have chosen a specific GPU. This will completely '
| distributed init (rank 0): env://
ngpus_per_node 1
Use GPU: 0 for training
=> creating model 'resnet50'
DistributedDataParallel(
  (module): SimSiam(
    (encoder): ResNet(
      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (relu): ReLU(inplace=True)
      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
      (layer1): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer2): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer3): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (3): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (4): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (5): Bottleneck(
          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (layer4): Sequential(
        (0): Bottleneck(
          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
          (downsample): Sequential(
            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (1): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
        (2): Bottleneck(
          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          (relu): ReLU(inplace=True)
        )
      )
      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
      (fc): Sequential(
        (0): Linear(in_features=2048, out_features=2048, bias=False)
        (1): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): ReLU(inplace=True)
        (3): Linear(in_features=2048, out_features=2048, bias=False)
        (4): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (5): ReLU(inplace=True)
        (6): Linear(in_features=2048, out_features=2048, bias=True)
        (7): BatchNorm1d(2048, eps=1e-05, momentum=0.1, affine=False, track_running_stats=True)
      )
    )
    (predictor): Sequential(
      (0): Linear(in_features=2048, out_features=512, bias=False)
      (1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (2): ReLU(inplace=True)
      (3): Linear(in_features=512, out_features=2048, bias=True)
    )
  )
)
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Length of train_dataset =  126689
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
data_dir /scratch/leuven/346/vsc34686/datasets/Imagenet_downloads/
Epoch: [0][  0/625]	Time  3.442 ( 3.442)	Data  2.658 ( 2.658)	Loss 2.3758e+00 (2.3758e+00)
Epoch: [0][ 10/625]	Time  0.026 ( 0.336)	Data  0.006 ( 0.247)	Loss 1.8991e+00 (2.3330e+00)
Epoch: [0][ 20/625]	Time  0.030 ( 0.189)	Data  0.007 ( 0.132)	Loss 2.2706e+00 (2.0913e+00)
Epoch: [0][ 30/625]	Time  0.025 ( 0.136)	Data  0.006 ( 0.092)	Loss 2.0614e+00 (2.0683e+00)
Epoch: [0][ 40/625]	Time  0.027 ( 0.109)	Data  0.007 ( 0.071)	Loss 1.2465e+00 (1.9145e+00)
Epoch: [0][ 50/625]	Time  0.070 ( 0.095)	Data  0.006 ( 0.058)	Loss 1.1783e+00 (1.8609e+00)
Epoch: [0][ 60/625]	Time  0.025 ( 0.087)	Data  0.005 ( 0.051)	Loss 7.3688e-01 (1.7591e+00)
Epoch: [0][ 70/625]	Time  0.026 ( 0.078)	Data  0.006 ( 0.044)	Loss 9.3344e-01 (1.6972e+00)
Epoch: [0][ 80/625]	Time  0.025 ( 0.072)	Data  0.006 ( 0.040)	Loss 1.2354e+00 (1.6267e+00)
Epoch: [0][ 90/625]	Time  0.025 ( 0.067)	Data  0.006 ( 0.036)	Loss 8.7761e-01 (1.5525e+00)
Epoch: [0][100/625]	Time  0.025 ( 0.063)	Data  0.006 ( 0.033)	Loss 8.9883e-01 (1.4701e+00)
Epoch: [0][110/625]	Time  0.025 ( 0.059)	Data  0.006 ( 0.031)	Loss 7.1377e-01 (1.4049e+00)
Epoch: [0][120/625]	Time  0.025 ( 0.057)	Data  0.006 ( 0.029)	Loss 6.7253e-01 (1.3499e+00)
Epoch: [0][130/625]	Time  0.026 ( 0.054)	Data  0.007 ( 0.027)	Loss 2.7131e-01 (1.2897e+00)
Epoch: [0][140/625]	Time  0.025 ( 0.052)	Data  0.006 ( 0.025)	Loss 5.5916e-01 (1.2335e+00)
Epoch: [0][150/625]	Time  0.027 ( 0.051)	Data  0.007 ( 0.024)	Loss 3.2576e-01 (1.1754e+00)
Epoch: [0][160/625]	Time  0.025 ( 0.049)	Data  0.007 ( 0.023)	Loss 4.1810e-01 (1.1195e+00)
Epoch: [0][170/625]	Time  0.027 ( 0.048)	Data  0.007 ( 0.022)	Loss 6.5731e-01 (1.0723e+00)
Epoch: [0][180/625]	Time  0.025 ( 0.046)	Data  0.006 ( 0.021)	Loss 2.7098e-01 (1.0226e+00)
Epoch: [0][190/625]	Time  0.026 ( 0.045)	Data  0.007 ( 0.021)	Loss 3.7932e-01 (9.8790e-01)
Epoch: [0][200/625]	Time  0.031 ( 0.044)	Data  0.006 ( 0.020)	Loss 1.4346e-01 (9.5179e-01)
Epoch: [0][210/625]	Time  0.027 ( 0.044)	Data  0.006 ( 0.019)	Loss 1.0593e-01 (9.1795e-01)
Epoch: [0][220/625]	Time  0.026 ( 0.043)	Data  0.007 ( 0.019)	Loss 3.0719e-01 (8.8420e-01)
Epoch: [0][230/625]	Time  0.026 ( 0.042)	Data  0.006 ( 0.018)	Loss 2.5925e-01 (8.5157e-01)
Epoch: [0][240/625]	Time  0.031 ( 0.041)	Data  0.007 ( 0.018)	Loss 2.2117e-01 (8.2476e-01)
Epoch: [0][250/625]	Time  0.026 ( 0.041)	Data  0.007 ( 0.017)	Loss 1.4536e-01 (7.9659e-01)
Epoch: [0][260/625]	Time  0.036 ( 0.040)	Data  0.007 ( 0.017)	Loss 1.2819e-01 (7.7278e-01)
Epoch: [0][270/625]	Time  0.029 ( 0.040)	Data  0.007 ( 0.016)	Loss 6.6861e-02 (7.4612e-01)
Epoch: [0][280/625]	Time  0.025 ( 0.040)	Data  0.007 ( 0.016)	Loss 1.9828e-01 (7.2418e-01)
Epoch: [0][290/625]	Time  0.027 ( 0.039)	Data  0.007 ( 0.016)	Loss 5.8615e-02 (7.0256e-01)
Epoch: [0][300/625]	Time  0.025 ( 0.039)	Data  0.006 ( 0.015)	Loss 1.8937e-01 (6.8359e-01)
Epoch: [0][310/625]	Time  0.027 ( 0.038)	Data  0.007 ( 0.015)	Loss 1.6376e-01 (6.6871e-01)
Epoch: [0][320/625]	Time  0.026 ( 0.038)	Data  0.007 ( 0.015)	Loss 4.6620e-02 (6.5069e-01)
Epoch: [0][330/625]	Time  0.025 ( 0.038)	Data  0.004 ( 0.015)	Loss 1.7413e-01 (6.3424e-01)
Epoch: [0][340/625]	Time  0.026 ( 0.038)	Data  0.007 ( 0.015)	Loss 6.6797e-02 (6.1827e-01)
Epoch: [0][350/625]	Time  0.025 ( 0.037)	Data  0.006 ( 0.014)	Loss 4.3676e-02 (6.0250e-01)
Epoch: [0][360/625]	Time  0.027 ( 0.037)	Data  0.007 ( 0.014)	Loss 3.7365e-02 (5.8744e-01)
Epoch: [0][370/625]	Time  0.025 ( 0.037)	Data  0.006 ( 0.014)	Loss 1.6961e-01 (5.7312e-01)
Epoch: [0][380/625]	Time  0.027 ( 0.037)	Data  0.007 ( 0.014)	Loss 5.0914e-02 (5.5933e-01)
Epoch: [0][390/625]	Time  0.026 ( 0.037)	Data  0.006 ( 0.014)	Loss 2.4454e-02 (5.4710e-01)
Epoch: [0][400/625]	Time  0.028 ( 0.036)	Data  0.006 ( 0.014)	Loss 5.3171e-02 (5.3481e-01)
Epoch: [0][410/625]	Time  0.027 ( 0.036)	Data  0.006 ( 0.013)	Loss 1.1482e-01 (5.2304e-01)
Epoch: [0][420/625]	Time  0.030 ( 0.036)	Data  0.006 ( 0.013)	Loss 1.6701e-01 (5.1221e-01)
Epoch: [0][430/625]	Time  0.026 ( 0.036)	Data  0.006 ( 0.013)	Loss 2.6562e-02 (5.0246e-01)
Epoch: [0][440/625]	Time  0.031 ( 0.036)	Data  0.006 ( 0.013)	Loss 3.3047e-02 (4.9189e-01)
Epoch: [0][450/625]	Time  0.030 ( 0.036)	Data  0.006 ( 0.013)	Loss 7.7300e-02 (4.8419e-01)
Epoch: [0][460/625]	Time  0.027 ( 0.035)	Data  0.006 ( 0.012)	Loss 6.9423e-02 (4.7459e-01)
Epoch: [0][470/625]	Time  0.026 ( 0.035)	Data  0.006 ( 0.012)	Loss 4.2511e-02 (4.6562e-01)
Epoch: [0][480/625]	Time  0.027 ( 0.035)	Data  0.006 ( 0.012)	Loss 3.6451e-02 (4.5736e-01)
Epoch: [0][490/625]	Time  0.027 ( 0.035)	Data  0.006 ( 0.012)	Loss 3.4946e-02 (4.4949e-01)
Epoch: [0][500/625]	Time  0.026 ( 0.035)	Data  0.006 ( 0.012)	Loss 6.4705e-02 (4.4153e-01)
Epoch: [0][510/625]	Time  0.027 ( 0.035)	Data  0.006 ( 0.012)	Loss 3.3329e-02 (4.3388e-01)
Epoch: [0][520/625]	Time  0.028 ( 0.035)	Data  0.006 ( 0.012)	Loss 8.9439e-02 (4.2634e-01)
Epoch: [0][530/625]	Time  0.027 ( 0.034)	Data  0.005 ( 0.012)	Loss 2.6425e-02 (4.1918e-01)
Epoch: [0][540/625]	Time  0.027 ( 0.034)	Data  0.006 ( 0.011)	Loss 8.7577e-02 (4.1238e-01)
Epoch: [0][550/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.011)	Loss 2.4199e-01 (4.0666e-01)
Epoch: [0][560/625]	Time  0.028 ( 0.034)	Data  0.006 ( 0.011)	Loss 2.3515e-02 (4.0113e-01)
Epoch: [0][570/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.011)	Loss 5.8890e-02 (3.9489e-01)
Epoch: [0][580/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.011)	Loss 1.5779e-01 (3.8944e-01)
Epoch: [0][590/625]	Time  0.026 ( 0.034)	Data  0.006 ( 0.011)	Loss 5.8037e-02 (3.8407e-01)
Epoch: [0][600/625]	Time  0.026 ( 0.033)	Data  0.006 ( 0.011)	Loss 2.9917e-02 (3.7815e-01)
Epoch: [0][610/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.9091e-01 (3.7283e-01)
Epoch: [0][620/625]	Time  0.025 ( 0.033)	Data  0.007 ( 0.011)	Loss 1.9085e-02 (3.6755e-01)
Test: [  0/625]	Time  2.702 ( 2.702)	Loss 3.7394e-02 (3.7394e-02)
Test: [ 10/625]	Time  0.072 ( 0.280)	Loss 4.1214e-02 (4.1563e-02)
Test: [ 20/625]	Time  0.011 ( 0.158)	Loss 2.3183e-02 (3.4483e-02)
Test: [ 30/625]	Time  0.063 ( 0.116)	Loss 2.6149e-02 (3.2391e-02)
Test: [ 40/625]	Time  0.060 ( 0.092)	Loss 1.9181e-02 (3.0051e-02)
Test: [ 50/625]	Time  0.011 ( 0.078)	Loss 1.9244e-02 (3.3444e-02)
Test: [ 60/625]	Time  0.083 ( 0.072)	Loss 3.4653e-02 (3.4220e-02)
Test: [ 70/625]	Time  0.010 ( 0.066)	Loss 3.1751e-02 (3.5413e-02)
Test: [ 80/625]	Time  0.009 ( 0.061)	Loss 4.1920e-02 (3.5845e-02)
Test: [ 90/625]	Time  0.060 ( 0.057)	Loss 4.6220e-02 (3.6300e-02)
Test: [100/625]	Time  0.010 ( 0.053)	Loss 3.2940e-02 (3.5530e-02)
Test: [110/625]	Time  0.009 ( 0.050)	Loss 2.9986e-02 (3.5455e-02)
Test: [120/625]	Time  0.009 ( 0.048)	Loss 4.9144e-02 (3.6417e-02)
Test: [130/625]	Time  0.009 ( 0.046)	Loss 2.1421e-02 (3.5987e-02)
Test: [140/625]	Time  0.010 ( 0.044)	Loss 2.7713e-02 (3.5600e-02)
Test: [150/625]	Time  0.071 ( 0.043)	Loss 3.0593e-02 (3.5386e-02)
Test: [160/625]	Time  0.010 ( 0.042)	Loss 5.0681e-02 (3.5129e-02)
Test: [170/625]	Time  0.009 ( 0.040)	Loss 7.2133e-02 (3.5488e-02)
Test: [180/625]	Time  0.010 ( 0.039)	Loss 2.1546e-02 (3.5262e-02)
Test: [190/625]	Time  0.009 ( 0.038)	Loss 6.0354e-02 (3.5265e-02)
Test: [200/625]	Time  0.010 ( 0.037)	Loss 4.9244e-02 (3.5594e-02)
Test: [210/625]	Time  0.011 ( 0.037)	Loss 1.9673e-02 (3.5751e-02)
Test: [220/625]	Time  0.011 ( 0.036)	Loss 4.2562e-02 (3.5404e-02)
Test: [230/625]	Time  0.012 ( 0.035)	Loss 3.3856e-02 (3.5217e-02)
Test: [240/625]	Time  0.010 ( 0.035)	Loss 2.3851e-02 (3.5187e-02)
Test: [250/625]	Time  0.010 ( 0.034)	Loss 2.0647e-02 (3.4992e-02)
Test: [260/625]	Time  0.009 ( 0.034)	Loss 4.4351e-02 (3.5098e-02)
Test: [270/625]	Time  0.010 ( 0.034)	Loss 2.6882e-02 (3.4982e-02)
Test: [280/625]	Time  0.011 ( 0.033)	Loss 4.0292e-02 (3.5002e-02)
Test: [290/625]	Time  0.077 ( 0.033)	Loss 3.6162e-02 (3.5031e-02)
Test: [300/625]	Time  0.069 ( 0.032)	Loss 4.9184e-02 (3.5177e-02)
Test: [310/625]	Time  0.011 ( 0.032)	Loss 2.9022e-02 (3.5178e-02)
Test: [320/625]	Time  0.011 ( 0.032)	Loss 3.3757e-02 (3.5463e-02)
Test: [330/625]	Time  0.060 ( 0.032)	Loss 7.4399e-02 (3.5740e-02)
Test: [340/625]	Time  0.010 ( 0.031)	Loss 2.7452e-02 (3.5808e-02)
Test: [350/625]	Time  0.010 ( 0.031)	Loss 2.7875e-02 (3.5782e-02)
Test: [360/625]	Time  0.010 ( 0.031)	Loss 3.1958e-02 (3.5833e-02)
Test: [370/625]	Time  0.009 ( 0.031)	Loss 5.9732e-02 (3.5657e-02)
Test: [380/625]	Time  0.070 ( 0.030)	Loss 2.6872e-02 (3.5613e-02)
Test: [390/625]	Time  0.075 ( 0.030)	Loss 2.1395e-02 (3.5981e-02)
Test: [400/625]	Time  0.010 ( 0.030)	Loss 2.5996e-02 (3.5815e-02)
Test: [410/625]	Time  0.011 ( 0.030)	Loss 4.5607e-02 (3.5737e-02)
Test: [420/625]	Time  0.010 ( 0.030)	Loss 5.7331e-02 (3.5689e-02)
Test: [430/625]	Time  0.083 ( 0.030)	Loss 3.1640e-02 (3.5824e-02)
Test: [440/625]	Time  0.084 ( 0.030)	Loss 3.6468e-02 (3.5703e-02)
Test: [450/625]	Time  0.010 ( 0.030)	Loss 3.0734e-02 (3.6175e-02)
Test: [460/625]	Time  0.009 ( 0.029)	Loss 2.3352e-02 (3.5960e-02)
Test: [470/625]	Time  0.016 ( 0.029)	Loss 3.1245e-02 (3.6053e-02)
Test: [480/625]	Time  0.010 ( 0.029)	Loss 3.6957e-02 (3.6114e-02)
Test: [490/625]	Time  0.009 ( 0.029)	Loss 3.1673e-02 (3.6123e-02)
Test: [500/625]	Time  0.009 ( 0.029)	Loss 2.9347e-02 (3.5952e-02)
Test: [510/625]	Time  0.078 ( 0.029)	Loss 2.6149e-02 (3.5823e-02)
Test: [520/625]	Time  0.016 ( 0.029)	Loss 5.6804e-02 (3.5746e-02)
Test: [530/625]	Time  0.010 ( 0.028)	Loss 2.5800e-02 (3.5729e-02)
Test: [540/625]	Time  0.009 ( 0.028)	Loss 3.4808e-02 (3.5654e-02)
Test: [550/625]	Time  0.010 ( 0.028)	Loss 8.6960e-02 (3.5667e-02)
Test: [560/625]	Time  0.009 ( 0.028)	Loss 2.0255e-02 (3.5823e-02)
Test: [570/625]	Time  0.009 ( 0.028)	Loss 2.6146e-02 (3.5793e-02)
Test: [580/625]	Time  0.012 ( 0.028)	Loss 7.2865e-02 (3.6003e-02)
Test: [590/625]	Time  0.009 ( 0.027)	Loss 3.7201e-02 (3.6082e-02)
Test: [600/625]	Time  0.009 ( 0.027)	Loss 1.7318e-02 (3.5873e-02)
Test: [610/625]	Time  0.009 ( 0.027)	Loss 7.2842e-02 (3.5834e-02)
Test: [620/625]	Time  0.009 ( 0.026)	Loss 2.0071e-02 (3.5894e-02)
args.start_epoch =  0
Epoch: [0][    0/15836]	Time  5.325 ( 5.325)	Data  4.423 ( 4.423)	Loss 0.0001 (0.0001)
Epoch: [0/1000]  [    0/15836]  eta: 23:25:39  loss: 0.000053 (0.000053)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 5.325797  data: 4.423157  max mem: 3613
Epoch: [0][   10/15836]	Time  0.125 ( 0.820)	Data  0.000 ( 0.402)	Loss -0.0077 (-0.0034)
Epoch: [0/1000]  [   10/15836]  eta: 3:36:17  loss: -0.003047 (-0.003400)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.819994  data: 0.402195  max mem: 4577
Epoch: [0][   20/15836]	Time  0.116 ( 0.523)	Data  0.000 ( 0.211)	Loss -0.0003 (-0.0010)
Epoch: [0/1000]  [   20/15836]  eta: 2:17:49  loss: -0.001679 (-0.000346)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.282737  data: 0.000077  max mem: 4577
Epoch: [0][   30/15836]	Time  0.119 ( 0.395)	Data  0.000 ( 0.143)	Loss -0.0118 (-0.0021)
Epoch: [0/1000]  [   30/15836]  eta: 1:44:03  loss: -0.001167 (-0.001601)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.161264  data: 0.000062  max mem: 4577
Epoch: [0][   40/15836]	Time  0.120 ( 0.330)	Data  0.000 ( 0.108)	Loss 0.0048 (-0.0018)
Epoch: [0/1000]  [   40/15836]  eta: 1:26:46  loss: -0.002347 (-0.001421)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.126725  data: 0.000056  max mem: 4577
Epoch: [0][   50/15836]	Time  0.129 ( 0.292)	Data  0.000 ( 0.087)	Loss -0.0011 (-0.0023)
Epoch: [0/1000]  [   50/15836]  eta: 1:16:40  loss: -0.001754 (-0.001885)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.130935  data: 0.000050  max mem: 4577
Epoch: [0][   60/15836]	Time  0.141 ( 0.266)	Data  0.000 ( 0.073)	Loss -0.0030 (-0.0029)
Epoch: [0/1000]  [   60/15836]  eta: 1:09:49  loss: -0.004685 (-0.002609)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.134178  data: 0.000051  max mem: 4577
Epoch: [0][   70/15836]	Time  0.141 ( 0.247)	Data  0.000 ( 0.063)	Loss 0.0028 (-0.0037)
Epoch: [0/1000]  [   70/15836]  eta: 1:04:50  loss: -0.009939 (-0.003422)  lr: 0.000937 (0.000938)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.132894  data: 0.000064  max mem: 4577
Epoch: [0][   80/15836]	Time  0.128 ( 0.232)	Data  0.000 ( 0.055)	Loss -0.0006 (-0.0043)
Epoch: [0/1000]  [   80/15836]  eta: 1:00:54  loss: -0.009529 (-0.004093)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.129318  data: 0.000070  max mem: 4577
Epoch: [0][   90/15836]	Time  0.127 ( 0.221)	Data  0.000 ( 0.049)	Loss -0.0215 (-0.0052)
Epoch: [0/1000]  [   90/15836]  eta: 0:57:51  loss: -0.014359 (-0.005117)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.126918  data: 0.000057  max mem: 4577
Epoch: [0][  100/15836]	Time  0.126 ( 0.211)	Data  0.000 ( 0.044)	Loss -0.0039 (-0.0063)
Epoch: [0/1000]  [  100/15836]  eta: 0:55:25  loss: -0.017483 (-0.006215)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.127926  data: 0.000053  max mem: 4577
Epoch: [0][  110/15836]	Time  0.119 ( 0.204)	Data  0.000 ( 0.040)	Loss -0.0099 (-0.0071)
Epoch: [0/1000]  [  110/15836]  eta: 0:53:28  loss: -0.017338 (-0.006893)  lr: 0.000937 (0.000937)  epoch: 0.000000 (0.000000)  weight_decay: 0.000100 (0.000100)  time: 0.129198  data: 0.000050  max mem: 4577
Traceback (most recent call last):
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 748, in <module>
    main()
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 173, in main
    main_worker(args.gpu, ngpus_per_node, args)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 393, in main_worker
    old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2 = train(train_loader, model, criterion, optimizer, epoch, args, memory_loader,val_loader, model_without_ddp, old_cv_p1, old_cv_z1, old_cv_p2, old_cv_z2,r_model,c_model, r_c_loss_avg)
  File "main_simsiam_imagenet100_ood_aug_remove_samples_during_train.py", line 470, in train
    p1, p2, z1, z2 = model(x1=images_0, x2=images_1)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/parallel/distributed.py", line 619, in forward
    output = self.module(*inputs[0], **kwargs[0])
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File ".././simsiam/builder.py", line 55, in forward
    z1 = self.encoder(x1) # NxC
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 220, in forward
    return self._forward_impl(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 211, in _forward_impl
    x = self.layer4(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torchvision/models/resnet.py", line 104, in forward
    out = self.conv1(x)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 423, in forward
    return self._conv_forward(input, self.weight)
  File "/data/leuven/346/vsc34686/anaconda3/envs/npy36/lib/python3.6/site-packages/torch/nn/modules/conv.py", line 420, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: CUDA out of memory. Tried to allocate 2.26 GiB (GPU 0; 23.87 GiB total capacity; 1.01 GiB already allocated; 21.48 GiB free; 1.69 GiB reserved in total by PyTorch)